Ultimate BMCP System Specification (Real-time, File Watcher Variant)
1. OVERARCHING GOAL & NON-NEGOTIABLE PRINCIPLES
Build the Broad Multi-language Code Parser (BMCP) system. This system must analyze large, complex, polyglot codebases by continuously monitoring the file system for changes, building a high-fidelity Neo4j knowledge graph of all code entities and their relationships, and supporting efficient, near real-time querying and visualization reflecting the latest code state.
Core Design Principles (Mandatory):
MAXIMUM ACCURACY & PRECISION: Correctly identifying all relevant code entities and their relationships (within/across files, languages, frameworks) is the absolute top priority. Minimize false positives and negatives.
REAL-TIME UPDATES: Changes detected on the file system MUST be processed and reflected in the Neo4j graph promptly (target low latency).
MASSIVE SCALABILITY: The architecture MUST handle large codebases efficiently, including the initial full scan and the continuous processing of file changes. This includes processing time, resource usage (CPU, memory, network), and database performance.
EXTENSIBILITY: Adding support for new languages, frameworks, or entity types MUST be straightforward, requiring minimal changes to the core infrastructure. Modularity is key.
CONSISTENCY: Canonical IDs and GIDs MUST be generated with perfect consistency across the entire system. The Centralized ID Service is mandatory.
RESILIENCE: The system should be robust against partial failures (e.g., parser errors on save, temporary FS/DB unavailability, network issues).
Initial Target Languages/Technologies (Must be Supported):
Go, Python, Java, C#, C++, Rust
TypeScript, JavaScript (Node.js, Deno)
HTML, CSS (including Tailwind context)
React, Preact (JSX/TSX parsing required)
SQL (multiple dialects), Supabase specific files
(Design for adding more easily)
2. MANDATED ARCHITECTURE & TECHNOLOGIES
To meet the principles above, the following architectural choices are mandatory:
Microservices: Decompose the system into specific services (ID Service, File Watcher, Analyzers per language/group, Ingestion Worker, API Gateway/Config Service).
Containerization: All services MUST be containerized using Docker.
File Watcher Service (MANDATORY): A dedicated service responsible for efficiently monitoring specified codebase directories using native OS mechanisms (e.g., inotify, FSEvents, ReadDirectoryChangesW). It must detect file creation, modification, and deletion events.
Communication Backbone: Message Queue (STRONGLY RECOMMENDED):
Use a robust message queue (e.g., Kafka, NATS, RabbitMQ) for decoupling and resilience.
Define queues/topics:
bmcp.events.filesystem: For raw file events from the Watcher (path, event type: CREATE/MODIFY/DELETE).
bmcp.jobs.analysis: For dispatching specific analysis tasks (file path, language, content/reference) based on debounced/processed file events.
bmcp.results.analysis: For analyzers to publish results (nodes, relationship stubs, potentially node/rel deletions).
bmcp.jobs.deletion: (Optional) Explicit queue for handling deletions identified by the watcher or analyzer.
Centralized Canonical ID / GID Service (MANDATORY):
Implement a dedicated, high-performance, resilient gRPC service (Node.js/TS or Rust recommended) as the single source of truth for generating all Canonical IDs and GIDs according to the specified standard (Section 3.A).
ALL other services needing IDs MUST call this service. NO local ID logic.
Stateless, Scalable Services: All Language Analyzers and Neo4j Ingestion Workers MUST be stateless and horizontally scalable.
Efficient Neo4j Updates: Implement logic for incremental create, update, and delete operations in Neo4j, reflecting file changes accurately. Retain the concept of two-phase resolution (adapted for real-time, see Section 3.C) to handle dependencies robustly.
3. CORE REQUIREMENTS (Detailed)
3.A. Canonical Identifier (Canonical ID) & Global ID (GID) System
Requirement: Implement the Centralized gRPC ID Service (See Section 2). It must perfectly adhere to the following standard:
Canonical ID Structure: :: delimited. Must include normalized relative path (relative to the monitored root), standardized & extensible Entity Type, entity name, parent context (using parent's Canonical ID), and parameter signature () for functions/methods. Handle specific types for new languages/frameworks (JSX Components, CSS Rules, HTML Elements, Config Settings, etc.). Apply name sanitization (:: -> __). Reference shared/canonical-ids/src/index.ts logic but implement centrally. Provide ParseCanonicalId via gRPC.
GID Structure: {language_prefix}:{sha256_of_canonical_id} generated by the central service. Use standard language prefixes (extensible). GID is the immutable Neo4j primary key.
Central Service gRPC Interface:
rpc GenerateId(GenerateIdRequest) returns (GenerateIdResponse)
GenerateIdRequest: Contains all context (filePath, entityType, name, parentCanonicalId, paramTypes, languageHint, etc.)
GenerateIdResponse: { canonical_id: string, gid: string }
rpc ParseId(ParseIdRequest) returns (ParseIdResponse)
ParseIdRequest: { id_string: string }
ParseIdResponse: Detailed components of the ID.
3.B. Relationship Identification & Accuracy
Requirement: Maximize relationship accuracy, minimizing false positives/negatives.
Parsers: Use Tree-sitter with high-quality grammars wherever possible. Augment with native parsers/compiler APIs only when necessary. Must handle partially correct code gracefully (e.g., during live editing).
Extraction: Analyzers MUST identify a comprehensive, extensible set of entities and relationships (:DEFINES, :CONTAINS, :IMPORTS, :USES_IMPORT, :CALLS, :INSTANTIATES, :INHERITS_FROM, :IMPLEMENTS, :REFERENCES, etc.). For modified files, analyzers should ideally detect not only the new state but also what entities/relationships were removed or changed compared to the previous state (potentially via AST diffing or re-analysis and comparison).
Cross-Reference Handling: Relationships spanning files/languages are represented as stubs { source_gid, target_canonical_id, type, properties } in analyzer output. Target Canonical ID generation requires best-effort, language-aware import resolution.
Hints (Required): Analyzers MUST parse source code comments like # bmcp:call-target <TargetCanonicalID> or # bmcp:relationship <Type> <TargetCanonicalID> to create relationship stubs.
Type System Integration (Recommended): Leverage type information (TS, Java, C#, Typed Python, Go) to improve accuracy.
3.C. Neo4j Ingestion & Adapted Two-Phase Resolution (for Real-time)
Requirement: Robustly and efficiently update the graph based on file changes.
Ingestion Workers: Consume from bmcp.results.analysis (and potentially bmcp.jobs.deletion). MUST process messages, ideally batching small updates that arrive close together.
Incremental Updates:
Nodes: MERGE (n:Node {gid: row.gid}) ON CREATE SET n += row.properties ON MATCH SET n = row.properties (Use SET n = for full overwrite on modify, or SET n += if merging properties is desired). Apply labels dynamically. Store canonical_id.
Relationships (New/Modified): Stored initially as pending relationships.
Deletions: MUST handle deletion of nodes (based on GIDs identified by analyzer/watcher for deleted files or entities) and associated relationships. Use DETACH DELETE n.
Adapted Two-Phase Resolution (Near Real-time):
Pending Relationship Creation: When an analysis result indicates a relationship from source_gid to target_canonical_id, create/update a :PendingRelationship node/structure associated with source_gid.
Immediate Resolution Attempt (Source Side): During the same transaction, check if a node with target_canonical_id already exists in the graph. If yes, create the final relationship (s)-[r]->(t) immediately and do not persist the pending relationship (or remove it if updating).
Immediate Resolution Attempt (Target Side): When a node t is created or updated (via MERGE), check if any :PendingRelationship exists that points to t.canonical_id. If yes, resolve those relationships immediately by creating the final link and deleting the pending stub(s).
(Fallback): Periodic background job (or triggered after quiet periods) to find and resolve any remaining :PendingRelationship nodes (necessary for cases where source/target are processed non-atomically or out of order despite efforts).
Neo4j Setup:
Indexes (Mandatory): gid (unique constraint), canonical_id (index). Add others (name, filePath, language) as needed for query performance.
Performance Tuning: Crucial for handling frequent small writes/reads.
3.D. File System Monitoring & Processing
Requirement: Efficiently detect and process relevant file changes.
File Watcher Service:
Use OS-native file system notification APIs (inotify, FSEvents, ReadDirectoryChangesW).
Monitor specified directories recursively.
Filter events based on configuration (e.g., ignore node_modules, .git, specific extensions).
Debounce/Throttle: Group rapid changes to the same file to avoid redundant analysis triggers.
Publish change events (path, type) to bmcp.events.filesystem or directly trigger analysis jobs.
Identify file language based on extension or content analysis.
Handle edge cases (renames, moves, directory changes).
Analysis Triggering: Logic (can be in Watcher or a dedicated processing service consuming from bmcp.events.filesystem) decides when to trigger analysis based on debounced events. Publishes job to bmcp.jobs.analysis. For MODIFY events, it MUST provide file content or path; for DELETE, it signals a deletion.
4. SERVICE SPECIFICATIONS (Key Aspects)
4.A. Central ID Service (gRPC)
Technology: Node.js/TypeScript or Rust.
Responsibilities: Sole provider of Canonical ID / GID generation and parsing.
API: Implement GenerateId and ParseId gRPC methods (Section 3.A).
Requirements: High availability, low latency, perfect consistency.
4.B. File Watcher Service
Technology: Language with good OS integration (e.g., Go, Rust, Node.js, Python with appropriate libraries).
Responsibilities: Monitor FS, filter events, debounce/throttle, determine language, publish analysis jobs or raw events to queue.
Requirements: Efficient, low-resource usage, configurable (paths, ignores), resilient to FS issues.
4.C. Language Analyzers (One Service per Lang/Group)
Responsibilities: Consume analysis jobs from queue, read file content, parse file, handle partial/invalid syntax gracefully, call Central ID Service, identify added/modified/deleted entities/relationships within the file, perform import resolution, parse hints, publish detailed results (including deletions) to results queue.
Technology: Language-appropriate. Tree-sitter focus recommended.
Requirements: Stateless, horizontally scalable, fast execution, accurate delta detection (if possible), strictly use Central ID Service.
4.D. Neo4j Ingestion Worker(s) (Python/Go/Node.js)
Responsibilities: Consume from results queue, batch process results, execute incremental Neo4j updates (create/update/delete nodes and relationships) using optimized Cypher transactions. Handle relationship resolution attempts (Section 3.C).
Requirements: Stateless, horizontally scalable, efficient batch processing, robust transactional logic for updates/deletions.
4.E. API Gateway / Configuration Service (FastAPI/Node.js)
Responsibilities: Provide REST API for:
Configuration management (monitored paths, ignored patterns, language settings).
System status monitoring (watcher status, queue depths, analyzer health).
Triggering initial full codebase scan (optional but useful).
Proxying queries to Neo4j for visualization/clients.
(Potentially) Authentication/Authorization.
(Potentially) Serving a simple admin dashboard UI.
Requirements: User-friendly configuration, clear status reporting.
5. DATA MODELS & CONTRACTS
Message Queue Payloads: Define clear JSON or Protobuf schemas for messages (File Events, Analysis Jobs, Analysis Results including delta information).
Core Data Structures: (Largely same as previous spec, but results need to indicate potential deletions)
AnalysisNodeStub: { gid, canonical_id, name, file_path, language, labels, properties }
AnalysisRelationshipStub: { source_gid, target_canonical_id, type, properties }
AnalyzerResultPayload: { file_path, language, error?, nodes_upserted: List[AnalysisNodeStub], relationships_upserted: List[AnalysisRelationshipStub], nodes_deleted: List[string], relationships_deleted: List[?] } (Need structure for identifying deleted relationships)
gRPC / Protobuf: Define services and messages for Central ID Service. Internal communication might rely more on the message queue.
Neo4j Schema: As before: :Node + specific labels, gid (unique), canonical_id (indexed). Need robust relationship types. Pending relationship structure TBD (Nodes vs. properties).
6. TESTING FRAMEWORK (Leverage user's detailed points)
Unit Tests: Isolate parsers, graph algorithms, change detection logic, ID generation calls.
Integration Tests: End-to-end flow (file save -> watcher -> queue -> analyzer -> queue -> ingestor -> Neo4j update). Validate graph state vs. code state. Test across languages. Test deletion propagation.
Performance Tests: Measure initial scan time. Measure latency from file save to graph update under load (various file sizes, rapid changes, many concurrent changes). Benchmark Neo4j update/query performance. Measure memory/CPU usage.
Fault Tolerance Tests: Neo4j connection loss/recovery, FS access issues, malformed code handling, analyzer crashes, queue unavailability, partial update failures.
Accuracy Tests: Validate precision/recall of entities and relationships on complex sample projects.
7. DEPLOYMENT & OPERATIONS
Containerization (Docker): Mandatory.
Orchestration: K8s recommended. Provide manifests/Helm charts.
Configuration: Use environment variables, config files, and potentially the Config API.
CI/CD: Automated testing and deployment.
Monitoring: Essential! Metrics (queue depths, processing times, error rates), logs (structured), tracing across services. Alerting.
Initial Scan: Provide mechanism/script to trigger an analysis of all files in monitored paths on startup or on demand. Consider using neo4j-admin database load for the very first population for massive codebases before starting the watcher.
8. SUCCESS CRITERIA
Demonstrably high accuracy in identifying nodes/relationships across target languages.
File system changes are reliably detected and processed.
Neo4j graph updates occur promptly (low latency) after file changes are detected and processed.
System scales horizontally to handle large codebases and high rates of file modification.
New language support can be added via new Analyzer service + configuration.
System remains stable and resilient under continuous operation and potential error conditions.
9. CONCLUSION
Build the real-time BMCP system according to these specifications. Prioritize Accuracy, Real-time Update Latency, Scalability, and Extensibility. The mandated use of a File Watcher Service, Centralized ID Service, robust incremental Neo4j updates, and adapted two-phase resolution is critical. Implement comprehensive testing and monitoring. The result should be a powerful tool for understanding complex codebases as they evolve.